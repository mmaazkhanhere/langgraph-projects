{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geqbxDKea8uf",
        "outputId": "94c86c0a-5a66-468a-e9be-037aa9253b94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.0/125.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.9/108.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install  --quiet langgraph\n",
        "!pip install  --quiet langchain-google-genai\n",
        "!pip install --quiet langchain\n",
        "!pip install --quiet tavily-python\n",
        "!pip install --quiet langchain_community\n",
        "!pip install --quiet langchain-groq\n",
        "!pip install --quiet wikipedia\n",
        "!pip install --quiet google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbeKuTKIb-9y"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "from IPython.display import Image, display, Markdown\n",
        "import textwrap\n",
        "import os\n",
        "import getpass\n",
        "import time\n",
        "\n",
        "from typing import Any, Annotated, List, TypedDict\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import get_buffer_string, AIMessage, HumanMessage, SystemMessage\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_community.document_loaders import WikipediaLoader\n",
        "from langchain_community.retrievers import WikipediaRetriever\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "from langgraph.graph import MessagesState\n",
        "from langgraph.graph.state import StateGraph, END, START\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph.message import add_messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HaLc7G7Bccnw"
      },
      "outputs": [],
      "source": [
        "# google_api_key = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\",\n",
        "#                               api_key=google_api_key\n",
        "#                               )\n",
        "\n",
        "# # model.invoke(\"Test\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = userdata.get(\"GROQ_API_KEY\")\n",
        "\n",
        "model = ChatGroq(\n",
        "    model=\"llama-3.2-1b-preview\",\n",
        "    verbose=True,\n",
        "    temperature=0.5,\n",
        "    api_key=api_key\n",
        ")\n",
        "\n",
        "model.invoke('TEST').content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "J_-kSLwzLUI2",
        "outputId": "6c2d0ff3-4fb2-4814-e1ed-6215c51e5fdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SecretNotFoundError",
          "evalue": "Secret GROQ_API_KEY does not exist.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSecretNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-017353c1d6c8>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GROQ_API_KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m model = ChatGroq(\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"llama-3.2-1b-preview\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/userdata.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exists'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mSecretNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'access'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotebookAccessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSecretNotFoundError\u001b[0m: Secret GROQ_API_KEY does not exist."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CLl7rIMH_ij"
      },
      "outputs": [],
      "source": [
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "\n",
        "_set_env(\"TAVILY_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQinv6poc4bB"
      },
      "outputs": [],
      "source": [
        "memory = MemorySaver()\n",
        "\n",
        "class State(TypedDict):\n",
        "  topic: str\n",
        "  pro_debator: str\n",
        "  anti_debator: str\n",
        "  greetings: str\n",
        "  analysis: str\n",
        "  pro_debator_response: str\n",
        "  anti_debator_response: str\n",
        "  context: Annotated[list, add_messages]\n",
        "  debate: Annotated[list, add_messages]\n",
        "  debate_history: List[str]\n",
        "  iteration: int\n",
        "  max_iteration: int\n",
        "\n",
        "class SearchQuery(BaseModel):\n",
        "  search_query: str = Field(description=\"The search query for retrieval\")\n",
        "\n",
        "structure_llm = model.with_structured_output(SearchQuery)\n",
        "structure_llm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def measure_time(node_function):\n",
        "    \"\"\"Decorator to measure and log the execution time of a node function.\"\"\"\n",
        "    def wrapper(state, *args, **kwargs):\n",
        "        start_time = time.time()\n",
        "        print(f\"Starting node: {node_function.__name__}\")\n",
        "        result = node_function(state, *args, **kwargs)\n",
        "        end_time = time.time()\n",
        "        elapsed_time = end_time - start_time\n",
        "        print(f\"Node {node_function.__name__} completed in {elapsed_time:.2f} seconds.\\n\")\n",
        "\n",
        "        # Optionally store in state for later analysis\n",
        "        if \"node_times\" not in state:\n",
        "            state[\"node_times\"] = {}\n",
        "        state[\"node_times\"][node_function.__name__] = elapsed_time\n",
        "\n",
        "        return result\n",
        "    return wrapper\n"
      ],
      "metadata": {
        "id": "qOgKP16PKLUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WwmmYvGeA8w"
      },
      "outputs": [],
      "source": [
        "@measure_time\n",
        "def greeting_node(state: State):\n",
        "  \"\"\"LangGraph node that greets the debators and introduces them\"\"\"\n",
        "  print(\"Greeting Node\")\n",
        "  topic = state['topic']\n",
        "  pro_debator = state['pro_debator']\n",
        "  anti_debator = state['anti_debator']\n",
        "\n",
        "  prompt = f\"\"\"You are hosting a debate between {pro_debator} and {anti_debator}\n",
        "            on the topic {topic}. {pro_debator} is pro while {anti_debator} is\n",
        "            against. You have to introduce the topic and debators to the audience.\n",
        "            Your response should be short and conversational\n",
        "            \"\"\"\n",
        "\n",
        "  greetings = model.invoke(prompt).content\n",
        "  return {\"greetings\": greetings}\n",
        "\n",
        "\n",
        "@measure_time\n",
        "def analyzer_node(state: State):\n",
        "    \"\"\"LangGraph node that analyzes the latest argument for web search\"\"\"\n",
        "    print(\"Analyzer Node\")\n",
        "    topic = state['topic']\n",
        "    debate = state['debate']\n",
        "    pro_debator = state['pro_debator']\n",
        "    anti_debator = state['anti_debator']\n",
        "    last_message = debate[-1]\n",
        "    analysis_prompt = None\n",
        "    if isinstance(last_message, HumanMessage):\n",
        "        # Generate a prompt for a HumanMessage (pro-debator)\n",
        "        print(\"Analyzing for Anti Debator\")\n",
        "        analysis_prompt = f\"\"\"\n",
        "        Analyze the latest argument made by the pro-debator {pro_debator}  on the topic \"{topic}\".\n",
        "        Focus on its strengths, weaknesses, and logical coherence. Write a short and concise\n",
        "        analytical guidance that can be used for web search to help {anti_debator} better answer the argument\n",
        "        and more completely support their stance on the topic {topic}. Keep the analysis as short as possible\n",
        "        without losing quality.\n",
        "        **Pro-Debator's Argument:**\n",
        "        {last_message.content}\n",
        "        \"\"\"\n",
        "\n",
        "    elif isinstance(last_message, AIMessage):\n",
        "        # Generate a prompt for an AIMessage (anti-debator)\n",
        "        print(\"Analyzing for Pro Debator\")\n",
        "        analysis_prompt = f\"\"\"\n",
        "        Analyze the latest counterargument made by the anti-debator {anti_debator} on the topic \"{topic}\".\n",
        "        Identify key points of contention and evaluate their validity.  Write a short and concise\n",
        "        analytical guidance that can be used for web search to help {anti_debator} to effectively refute these arguments\n",
        "        and more completely support their stance on the topic {topic}. Keep the analysis as short as possible\n",
        "        without losing quality.\n",
        "        **Anti-Debator's Counterargument:**\n",
        "        {last_message.content}\n",
        "        \"\"\"\n",
        "\n",
        "    analysis = model.invoke(analysis_prompt).content\n",
        "    return {\"analysis\": analysis}\n",
        "\n",
        "\n",
        "@measure_time\n",
        "def search_web(state: State):\n",
        "    \"\"\"LangGraph node to search the web using Tavily Search API and append the results to context.\"\"\"\n",
        "    analysis = state['analysis']\n",
        "\n",
        "    context = state['context']\n",
        "\n",
        "    # Generate Search Query\n",
        "    search_query = model.invoke(\n",
        "        f\"Generate a web search query using analysis {analysis} and debate history {state['debate_history']}. the search query should be no longer than 3 sentences\"\n",
        "    ).content\n",
        "    print(\"Tavily Search Query:\", search_query)\n",
        "    tavily_search = TavilySearchResults(\n",
        "                      max_results=2,\n",
        "                      include_answer=True,\n",
        "                      include_raw_content=True,\n",
        "                      # search_depth=\"advanced\",\n",
        "                      # include_domains = []\n",
        "                      # exclude_domains = []\n",
        "                  )\n",
        "    search_docs = tavily_search.invoke(search_query)\n",
        "    print(\"search_docs:\", search_docs)\n",
        "\n",
        "    # Check if `search_docs` contains valid dictionaries\n",
        "    if isinstance(search_docs, list) and all(isinstance(doc, dict) for doc in search_docs):\n",
        "        formatted_search_docs = \"\\n\\n---\\n\\n\".join(\n",
        "            [\n",
        "                f\"**URL:** {doc.get('url', 'No URL')}\\n**Content:** {doc.get('content', 'No Content')}\"\n",
        "                for doc in search_docs\n",
        "            ]\n",
        "        )\n",
        "    elif isinstance(search_docs, list) and all(isinstance(doc, str) for doc in search_docs):\n",
        "        formatted_search_docs = \"\\n\\n---\\n\\n\".join(search_docs)\n",
        "    else:\n",
        "        formatted_search_docs = \"Search results are in an unexpected format.\"\n",
        "\n",
        "    # Append to context\n",
        "    context.append(formatted_search_docs)\n",
        "    return {\"context\": context}\n",
        "\n",
        "\n",
        "\n",
        "@measure_time\n",
        "def search_wikipedia(state: State):\n",
        "    \"\"\"Retrieve docs from Wikipedia using WikipediaRetriever\"\"\"\n",
        "    print(\"Searching Wikipedia\")\n",
        "\n",
        "    # Analysis and debate context\n",
        "    analysis = state['analysis']\n",
        "    debate_history = state['debate_history']\n",
        "    search_query = model.invoke(\n",
        "        f\"Generate a wikipedia search query using analysis {analysis} and debate history {state['debate_history']}. the search query should be no longer than 3 sentences\"\n",
        "    ).content\n",
        "    print(\"Wikipedia Search Query:\", search_query)\n",
        "\n",
        "    # WikipediaRetriever setup\n",
        "    retriever = WikipediaRetriever()\n",
        "    search_docs = retriever.get_relevant_documents(search_query)\n",
        "\n",
        "    # Format the results\n",
        "    formatted_search_docs = \"\\n\\n---\\n\\n\".join(\n",
        "        [\n",
        "            f\"<Document title='{doc.metadata.get('title', 'Unknown Title')}'/>\\n{doc.page_content}\\n</Document>\"\n",
        "            for doc in search_docs\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    print(f\"Wikipedia DOcs: {formatted_search_docs}\")\n",
        "    return {\"context\": [formatted_search_docs]}\n",
        "\n",
        "\n",
        "@measure_time\n",
        "def router(state: State):\n",
        "    \"\"\"LangGraph node that routes to the appropriate search function\"\"\"\n",
        "    debate_history = state[\"debate_history\"]\n",
        "    if debate_history == []:\n",
        "        return \"Pro Debator\"\n",
        "    else:\n",
        "      return \"Analyzer\"\n",
        "\n",
        "def iteration_router(state: State):\n",
        "    \"\"\"Routes the flow based on the current iteration and max_iteration\"\"\"\n",
        "\n",
        "    if state['iteration'] <= state['max_iteration']:\n",
        "        print(f\"Iteration Round: {state['iteration']}\")\n",
        "        state['iteration'] = state['iteration'] + 1\n",
        "        return \"Analyzer\"\n",
        "    else:\n",
        "        # End the debate\n",
        "        return END\n",
        "\n",
        "@measure_time\n",
        "def analyzer_router(state: State):\n",
        "    \"\"\"Function that routes to the appropriate next node\"\"\"\n",
        "    debate = state['debate']\n",
        "    last_message = debate[-1]\n",
        "    if isinstance(last_message, AIMessage):\n",
        "        return \"Pro Debator\"  # Pro Debator responds to the anti-debator's argument\n",
        "    else:\n",
        "        return \"Anti Debator\"  # Anti Debator responds to the pro-debator's argument\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I22jJGnD8k3T"
      },
      "outputs": [],
      "source": [
        "@measure_time\n",
        "def pro_debator_node(state: State):\n",
        "    \"\"\"LangGraph node that represents the pro debator\"\"\"\n",
        "\n",
        "    print(\"Pro Debator Node\")\n",
        "\n",
        "    topic = state['topic']\n",
        "    anti_debator_response = state['anti_debator_response']\n",
        "    pro_debator = state['pro_debator']\n",
        "    anti_debator = state['anti_debator']\n",
        "    debate_history = state['debate_history']\n",
        "    debate = state['debate']\n",
        "\n",
        "    if anti_debator_response is None and debate == []:\n",
        "        prompt_template = \"\"\"\n",
        "            You are {pro_debator}, a pro debator on the topic of {topic} having a debate with {anti_debator}.\n",
        "            Your goal is to present compelling arguments in favor of {topic} while maintaining the persona of {pro_debator}.\n",
        "            Ensure your responses are coherent, logical, and persuasive.\n",
        "            Keep the persona of {pro_debator} throughout the entire conversation.\n",
        "            Your responses should be relevant to the current stage of the debate.\n",
        "            You can refute the other debator's arguments and present your own supporting evidence for {topic}.\n",
        "            Do not deviate from your persona. Respond concisely and your response must be less than 4 sentences.\n",
        "        \"\"\"\n",
        "        system_message = prompt_template.format(topic=topic, pro_debator=pro_debator, anti_debator=anti_debator)\n",
        "        pro_debator_response_content = model.invoke(system_message).content\n",
        "    else:\n",
        "      context = state['context']\n",
        "      prompt_template = \"\"\"\n",
        "          You are a professional debater, embodying the persona of {pro_debator}. Your goal is to convincingly argue the affirmative side of the debate topic: \"{topic}\".\n",
        "          You must maintain your assigned persona throughout the debate and ensure that your arguments align with it.\n",
        "          Remember the following:\n",
        "          1. Respond to the latest argument of the anti-debator provided below, ensuring your response directly addresses their points.\n",
        "          2. Consider the context of the debate history {debate_history} and data gathered from web search {context}, building upon your\n",
        "          previous arguments and refuting the anti-debator's counterarguments effectively.\n",
        "          3. Use eloquent and persuasive language, demonstrating your mastery of the topic and your persona.\n",
        "          4. Avoid making factual errors or inconsistencies that might damage your credibility.\n",
        "          5. Response should be less than 5 sentences\n",
        "          **Debate History:**\n",
        "          {debate_history}\n",
        "          **Latest Anti-Debator Argument:**\n",
        "          {anti_debator_response}\n",
        "          **Your Response (Pro Debator):**\n",
        "      \"\"\"\n",
        "      system_message = prompt_template.format(\n",
        "          topic=topic,\n",
        "          pro_debator=pro_debator,\n",
        "          anti_debator=anti_debator,\n",
        "          debate_history=debate_history,\n",
        "          anti_debator_response=anti_debator_response,\n",
        "          context=context\n",
        "      )\n",
        "      pro_debator_response_content = model.invoke(system_message).content\n",
        "\n",
        "    # Create a HumanMessage with the response content and assign a name\n",
        "    pro_debator_response = HumanMessage(\n",
        "        content=f\"\"\"{pro_debator}: {pro_debator_response_content}\"\"\",\n",
        "        name=\"pro_response\"\n",
        "    )\n",
        "\n",
        "    debate.append(pro_debator_response)\n",
        "    return {\"pro_debator_response\": pro_debator_response, \"debate\": debate}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJGrpaQfQfI3"
      },
      "outputs": [],
      "source": [
        "@measure_time\n",
        "def anti_debator_node(state: State):\n",
        "    \"\"\"LangGraph node that represents the anti debator\"\"\"\n",
        "    print(\"Anti Debator Node\")\n",
        "    topic = state['topic']\n",
        "    anti_debator_response = state['anti_debator_response']\n",
        "    pro_debator = state['pro_debator']\n",
        "    anti_debator = state['anti_debator']\n",
        "    debate_history = state['debate_history']\n",
        "    debate = state['debate']\n",
        "    context = state['context']\n",
        "\n",
        "    prompt_template = prompt_template = \"\"\"\n",
        "            You are {anti_debator}, an anti debator on the topic of {topic} having a debate with {pro_debator}.\n",
        "            Your goal is to present compelling arguments against {pro_debator_response} on topic {topic} while maintaining the persona of {anti_debator}.\n",
        "            Ensure your responses are coherent, logical, and persuasive.\n",
        "            Keep the persona of {anti_debator} throughout the entire conversation.\n",
        "            Your responses should be relevant to the current stage of the debate.\n",
        "            You can refute the other debator's arguments and present your own supporting evidence against {topic}\n",
        "            using the context {context} and history of debate {debate_history}.\n",
        "            Do not deviate from your persona. Respond concisely in no more than 5 sentences.\n",
        "        \"\"\"\n",
        "    system_message = prompt_template.format(\n",
        "        topic=topic,\n",
        "        pro_debator=pro_debator,\n",
        "        pro_debator_response=anti_debator_response,\n",
        "        anti_debator=anti_debator,\n",
        "        debate_history=debate_history,\n",
        "        anti_debator_response=anti_debator_response,\n",
        "        context=context\n",
        "    )\n",
        "    pro_debator_response_content = model.invoke(system_message).content\n",
        "\n",
        "    # Create a HumanMessage with the response content and assign a name\n",
        "    anti_debator_response = AIMessage(\n",
        "        content=f\"\"\"{anti_debator}: {pro_debator_response_content}\"\"\",\n",
        "        name=\"pro_response\"\n",
        "    )\n",
        "\n",
        "    debate.append(anti_debator_response)\n",
        "    return {\"anti_debator_response\": anti_debator_response, \"debate\": debate}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhO8NyCwQg7e"
      },
      "outputs": [],
      "source": [
        "@measure_time\n",
        "def debate_summarizer_node(state: State):\n",
        "  \"\"\"LangGraph node that summarizes the exchange of arguments between debator\n",
        "  and append to history for future consideration\n",
        "  \"\"\"\n",
        "  pro_debator = state['pro_debator']\n",
        "  anti_debator = state['anti_debator']\n",
        "  debate_history = state['debate_history']\n",
        "  anti_debator_response = state['anti_debator_response']\n",
        "  pro_debator_response = state['pro_debator_response']\n",
        "  prompt = \"\"\"\n",
        "            Summarize the conversation between the pro {pro_debator} and anti debator {anti_debator},\n",
        "            highlighting the key points of their arguments and discarding unnecessary points. The\n",
        "            summary should be concise and brief, with high quality.\n",
        "            **Instructions:**\n",
        "            * Focus on the core arguments presented by both sides.\n",
        "            * Identify the main points of agreement and disagreement.\n",
        "            * Provide a clear and objective overview of the debate.\n",
        "            * Avoid including irrelevant details or repetitive information.\n",
        "            * Ensure that the summary is easy to understand and informative.\n",
        "            * The summary should be approximately 1.\n",
        "            **Pro Debator:**\n",
        "            {pro_debator_response}\n",
        "            **Anti Debator:**\n",
        "            {anti_debator_response}\n",
        "          \"\"\"\n",
        "  system_message = prompt.format(\n",
        "                      pro_debator=pro_debator,\n",
        "                      pro_debator_response=anti_debator_response,\n",
        "                      anti_debator=anti_debator,\n",
        "                      anti_debator_response=anti_debator_response,\n",
        "                    )\n",
        "  summary = model.invoke(system_message).content\n",
        "  debate_history.append(summary)\n",
        "  return {\"debate_history\": debate_history}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sm5Cm23LapZY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7VZ--S4uPrQ"
      },
      "outputs": [],
      "source": [
        "builder = StateGraph(State)\n",
        "\n",
        "# Add nodes\n",
        "builder.add_node(\"Greetings\", greeting_node)\n",
        "builder.add_node(\"Pro Debator\", pro_debator_node)\n",
        "builder.add_node(\"Analyzer\", analyzer_node)\n",
        "builder.add_node(\"Search Web\", search_web)\n",
        "builder.add_node(\"Search Wikipedia\", search_wikipedia)\n",
        "builder.add_node(\"Anti Debator\", anti_debator_node)\n",
        "builder.add_node(\"Debate Summarizer\", debate_summarizer_node)\n",
        "\n",
        "# Add edges\n",
        "builder.add_edge(START, \"Greetings\")\n",
        "builder.add_conditional_edges(\"Greetings\", router, ['Analyzer', 'Pro Debator'])\n",
        "builder.add_edge(\"Analyzer\", \"Search Web\")\n",
        "builder.add_edge(\"Analyzer\", \"Search Wikipedia\")\n",
        "builder.add_conditional_edges(\"Search Web\", analyzer_router, [\"Pro Debator\", \"Anti Debator\"])\n",
        "builder.add_conditional_edges(\"Search Wikipedia\", analyzer_router, [\"Pro Debator\", \"Anti Debator\"])\n",
        "builder.add_edge(\"Pro Debator\", \"Analyzer\")\n",
        "builder.add_edge(\"Anti Debator\", \"Debate Summarizer\")\n",
        "builder.add_edge(\"Debate Summarizer\", END)\n",
        "\n",
        "# Compile the graph\n",
        "debator = builder.compile(checkpointer=memory).with_config(run_name=\"Create podcast\")\n",
        "\n",
        "# Display the graph\n",
        "display(Image(debator.get_graph().draw_mermaid_png()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lg7mTyfHX1kD"
      },
      "outputs": [],
      "source": [
        "state = {\n",
        "    \"topic\": \"Ukraine War\",\n",
        "    \"pro_debator\": \"Joe Biden\",\n",
        "    \"anti_debator\": \"Donald Trump\",\n",
        "    \"greetings\": \"\",\n",
        "    \"analysis\": \"\",\n",
        "    \"pro_debator_response\": \"\",\n",
        "    \"anti_debator_response\": \"\",\n",
        "    \"context\": [],\n",
        "    \"debate\": [],\n",
        "    \"debate_history\": [],\n",
        "    \"iteration\": 0,\n",
        "    \"max_iteration\": 1\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgM_htTJu2kl"
      },
      "outputs": [],
      "source": [
        "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "result = debator.invoke(state, thread)\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tluh5bmpylTP"
      },
      "outputs": [],
      "source": [
        "import pprint\n",
        "pprint.pprint(result['pro_debator_response'].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjoPrUleZgbi"
      },
      "outputs": [],
      "source": [
        "pprint.pprint(result['anti_debator_response'].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAM-zmcjncup"
      },
      "outputs": [],
      "source": [
        "pprint.pprint(result['greetings'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lSByHOpno8M"
      },
      "outputs": [],
      "source": [
        "pprint.pprint(result['analysis'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUOaWKFenwoT"
      },
      "outputs": [],
      "source": [
        "result['debate']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E53U_X22p088"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kafddUtFTkWv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}